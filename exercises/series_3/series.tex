\documentclass[a4paper]{scrreprt}

% Uncomment to optimize for double-sided printing.
% \KOMAoptions{twoside}

% Set binding correction manually, if known.
% \KOMAoptions{BCOR=2cm}

% Localization options
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% Enhanced verbatim sections. We're mainly interested in
% \verbatiminput though.
\usepackage{verbatim}

% PDF-compatible landscape mode.
% Makes PDF viewers show the page rotated by 90Â°.
\usepackage{pdflscape}

% Advanced tables
\usepackage{tabu}
\usepackage{longtable}

% Fancy tablerules
\usepackage{booktabs}

% Graphics
\usepackage{graphicx}

% Current time
\usepackage[useregional=numeric]{datetime2}

% Float barriers.
% Automatically add a FloatBarrier to each \section
\usepackage[section]{placeins}

% Custom header and footer
\usepackage{fancyhdr}

\usepackage{geometry}
\usepackage{layout}

% Math tools
\usepackage{mathtools}
% Math symbols
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsthm}

\pagestyle{plain}
% \fancyhf{}
% \lhead{}
% \lfoot{}
% \rfoot{}
% 
% Source code & highlighting
\usepackage{listings}

% Convenience commands
\newcommand{\mailsubject}{451670- Algorithmen, Wahrscheinlichkeit, Informationen - Series 3}
\newcommand{\maillink}[1]{\href{mailto:#1?subject=\mailsubject}
                               {#1}}

% Should use this command wherever the print date is mentioned.
\newcommand{\printdate}{\today}

\subject{451670 - Algorithmen, Wahrscheinlichkeit, Informationen}
\title{Series 3}

\author{Michael Senn \maillink{michael.senn@students.unibe.ch} - 16-126-880}

\date{\printdate}

% Needs to be the last command in the preamble, for one reason or
% another. 
\usepackage{hyperref}


\begin{document}
\maketitle


\setcounter{chapter}{2}
\chapter{Series 3}

\section{Conditional expected value}

\subsection{$G_1$}

Let $X \sim Un(\{1, 2, 3, 4, 5, 6\})$ be the result of the die. Let $Y \sim
Un(\{1, 2, 3, 4, 5, 6\})$ be Alice's guess.

As such:

\[
	E[G_1] = P(X=Y) * 5 + P[X \neq Y] * -1 = \frac{1}{6} * 5 + \frac{5}{6} * -1 = 0
\]

\subsection{$G_2$}

Let $X_o \sim Un(\{1, 3, 5\})$ be the result of the odd die, $X_d \sim Un(\{2,
4, 6\})$ the result of the even die. Let $Y \sim Un(\{X_d, X_o\})$ whether Bob
chose the even or odd die. Let $Z$ be the result of the die throw. We first
show that $Z \sim Un(\{1, 2, 3, 4, 5, 6\})$, utilizing the independence of $Y$
and $X_i$ respectively.
\\

\begin{tabular}{|l|l|}
	\hline
	$z$ & $P[Z = z]$  \\
	\hline
	$1$ & $P[Y = X_o] * P[X_o = 1] = 1/6$ \\
	\hline
	$2$ & $P[Y = X_e] * P[X_e = 2] = 1/6$ \\
	\hline
	$3$ & $P[Y = X_o] * P[X_o = 3] = 1/6$ \\
	\hline
	$4$ & $P[Y = X_e] * P[X_e = 4] = 1/6$ \\
	\hline
	$5$ & $P[Y = X_o] * P[X_o = 5] = 1/6$ \\
	\hline
	$6$ & $P[Y = X_e] * P[X_e = 6] = 1/6$ \\
	\hline
\end{tabular}
\\
\\
As such, the result of the die is distributed identically to the die of the
previous exercise, hence $E[G_2] = E[G_1] = 0$.

\subsection{$G_3$}

Let $X \sim Un(\{a, b, c\})$ be the result of the die throw, with $a, b, c$
being either $1, 3, 5$ or $2, 4, 6$ with equal probability. Let $Y \sim Un(\{a,
b, c\})$ be Alice's guess, as she knows which set $a, b, c$ are chosen from.

As such:

\[
	E[G_3] = P(X = Y) * 5 + P(X \neq Y) * -1 = \frac{1}{3} * 5 + \frac{2}{3} * -1 = 1
\]


\section{Distributions}

\subsection{Couple, looking for girl}

Let $M = \text{Male}$, $F = \text{Female}$. Let $X_k$ be the number of children
conceived. Note first, for any given $k$, the possible combinations.
\\

\begin{tabular}{|l|l|}
	\hline
	Children & Probability \\
	\hline
	$\text{F}$ & $(1/2)^1$ \\
	\hline
	$\text{MF}$ & $(1/2)^1 * (1/2)^1 = (1/2)^2$ \\
	\hline
	$\text{MMF}$ & $(1/2)^2 * (1/2)^1 = (1/2)^3$ \\
	\hline
	$\text{M \ldots\ MF}$ & $(1/2)^{k-1} * (1/2)^1 = (1/2)^k$ \\
	\hline
	$\text{M \ldots\ M}$ & $(1/2)^{k}$ \\
	\hline
\end{tabular}
\\
\\
This allows us to derive our probability function:
\[
	P(X=x) =
	\begin{cases}
		\left(\frac{1}{2}\right)^x & x < k \\
		2 * \left(\frac{1}{2}\right)^x & x = k \\
		0 & x > k
	\end{cases}
\]

And from there our expected value:
\begin{align*}
	E[X] & = \sum_{x=1}^{\infty}{P(X=x) x} \\
	& = \sum_{x=1}^{k-1}{P(X=x) x} + P(X=k) k + \sum_{x=k+1}^{\infty}{P(X=x) x} \\
	& = \sum_{x=1}^{k-1}{\left(\frac{1}{2}\right)^x x} + 2 \left(\frac{1}{2}\right)^k k + \sum_{x=k+1}^{\infty}{0 x} \\
	& = 2^{1-k} (-k +2^k -1) + 2k \left(\frac{1}{2}\right)^k + 0 \\
	& = 2 - 2^{1-k}
\end{align*}

\subsection{One of both}

Let $M = \text{Male}$, $F = \text{Female}$. Let $X$ be the number of children
conceived. Note first that the couple will stop having kids as soon as they
have at least one boy and one girl. This means that the children, in order of
birth, will always have the form of either $\{M \ldots M, F\}$ or $\{F \ldots
F, M\}$. In words, an arbitrary number ($\geq 1$) of children of one gender,
followed by a single child of the opposite gender.

Let $Y$ be the number of either males or females which are born first, before
the one child of the opposite gender is born. $Y$ follows a geometric
distribution: $Y \sim Ge\left(\frac{1}{2}\right)$, and its expected value is
given as $E[Y] = \frac{1}{p} = 2$.

As seen above, the total number of children will be one higher - namely the
last child of opposite gender. As such:

\[
	E[X] = E[Y] + 1 = 3
\]

\subsection{Throwing coins until $k$ times head}

Let $X$ be the number of coin throws until a total of $k$ times head was
achieved. Note that $X=x$ implies that the last throw was head, with the
previous $x-1$ throws containing $k-1$ times head, and $(x-1)-(k-1)$ times
tails.

This means that the number of heads in the first $x-1$ throws follows a
binomial distribution: $Y \sim Bi(x-1, p)$.

As such, we can derive our probability function:
\begin{align*}
	P(X=x) & = P(Y = k-1) p \\
	       & = p^{k-1} (1-p)^{(x-1) - (k-1)} \binom{x-1}{k-1} p \\
	       & = p^k (1-p)^{x-k} \binom{n-1}{k-1}
\end{align*}

This implies that X follows a negative binomial distribution, whose expected value is given as:
\[
	E[X] = \frac{k}{p}
\]

\section{Traveling}

This can be solved by splitting $X$ into $X = A_1 + A_2 + ... + A_n$, with
$A_i$ being the number of days it takes him to travel to the i-th distinct
destination when having already visited $i-1$ distinct destinations.

Clearly, $A_1 = 1$, as no matter which train he picks on the first day, it is
guaranteed to lead him to a previously-unvisited destination. Afterwards, any
subsequent day has a chance of $\frac{n-1}{n}$ of leading him to a new
destination. Once he did arrive at the second distinct destination, any
subsequent day has a chance of $\frac{n-2}{n}$ of leading to yet another
destination, and so on.

As such, every $A_i$ follows the geometric distribution with $p =
\frac{n-(i-1)}{n}$, and $E[A_i] = \frac{n}{n-(i-1)}$. Hence:

\begin{align*}
	E[X] & = E\left[\sum_{i=1}^{n}{A_i}\right] \\
	     & = \sum_{i=1}^{n}{E[A_i]} \\
	     & = \sum_{i=1}^{n}{\frac{n}{n-i+1}} \\
	     & = \sum_{i=1}^{n}{\frac{n}{i}} \\
	     & = n * \sum_{i=1}^{n}{\frac{1}{i}}
\end{align*}

\section{Converting Monte-Carlo to Las-Vegas algorithm}

We can create a Las-Vegas algorithm as follows:

\begin{lstlisting}[mathescape=true]
LAS-VEGAS($\Pi$):
  DO
    solution := M($\Pi$)
  UNTIL V(solution)

  RETURN solution
\end{lstlisting}

Clearly this algorithm is guaranteed to provide a correct result, as it applies
$M$ onto the problem until the returned solution passes verification.
Furthermore, each iteration of the algorithm will take $y := T + K$ steps, as it
executes the Monte-Carlo algorithm $M$, followed by the verification algorithm
$V$.

Each iteration has a chance of $\alpha$ to produce a correct solution, so the
number of required iterations $X$ follows a geometric distribution with
probability $\alpha$: $X \sim Ge(\alpha)$.

This leads to a runtime of:
\[
  \text{Average runtime} = Y E[X] = (T + K) \frac{1}{\alpha} = \frac{T + K}{\alpha}
\]

\section{Online algorithm for random subset}

We show by induction that, after having read $n$ items $s_1 \ldots s_n$, the
chance of any $s_i$ being in the output $R := r_1 \ldots r_k$ is equal to
$\frac{k}{n}$.

\begin{proof}

  Let $R_n = r_1 \ldots r_k$ be the output after iteration $n$.

  Base case: $n := k$

  \[
    P(s_i \in R_n) = 1 = \frac{k}{n}\ \forall\ 1 \leq i \leq n = k
  \]

  Inductive step: $n \rightarrow n + 1$

  We aim to show that:
  \[
    P(s_i \in R_{n+1}) = \frac{k}{n+1}\ \forall\ 1 \leq i \leq n+1
  \]

  Let $1 \leq j \leq n+1$ be the random integer generated in the algorithm.

  For $s_{n+1}$ it is clear that it is part of $R_{n+1}$ iff it was swapped in,
  that is if $j \leq k$:
  \[
    P(s_{n+1} \in R) = P(j \leq k) = \frac{k}{n+1}
  \]

  For $s_i, 1 \leq i \leq n$ to be in $R_{n+1}$, it must have been in $R_{n}$,
  and must not have been swapped out during the current iteration.

  From the base case we know that:
  \[
    P(s_i) \in R_n = \frac{k}{n}
  \]

  $s_i$ is swapped out iff $j = i$, hence:
  \[
    P(s_i \text{ swapped out}) = \frac{1}{j} = \frac{1}{n+1}
  \]

  As such:
  \[
    P(s_i \in R_{n+1}) = P(s_i \in R_n \land s_i \text{ not swapped out}) = \frac{k}{n} \left(1 - \frac{1}{n+1}\right) = \frac{k}{n+1}
  \]

  Hence $r_1 \ldots r_k$ is a random uniform subset of $s_1 \ldots s_n$ $\forall n \geq k$.
\end{proof}

\end{document}

