\documentclass[a4paper]{scrreprt}

% Uncomment to optimize for double-sided printing.
% \KOMAoptions{twoside}

% Set binding correction manually, if known.
% \KOMAoptions{BCOR=2cm}

% Localization options
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% Enhanced verbatim sections. We're mainly interested in
% \verbatiminput though.
\usepackage{verbatim}

% PDF-compatible landscape mode.
% Makes PDF viewers show the page rotated by 90Â°.
\usepackage{pdflscape}

% Advanced tables
\usepackage{tabu}
\usepackage{longtable}

% Fancy tablerules
\usepackage{booktabs}

% Graphics
\usepackage{graphicx}

% Current time
\usepackage[useregional=numeric]{datetime2}

% Float barriers.
% Automatically add a FloatBarrier to each \section
\usepackage[section]{placeins}

% Custom header and footer
\usepackage{fancyhdr}

\usepackage{geometry}
\usepackage{layout}

% Math tools
\usepackage{mathtools}
% Math symbols
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsthm}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}

\pagestyle{plain}
% \fancyhf{}
% \lhead{}
% \lfoot{}
% \rfoot{}
% 
% Source code & highlighting
\usepackage{listings}

% Convenience commands
\newcommand{\mailsubject}{451670- Algorithmen, Wahrscheinlichkeit, Informationen - Series 5}
\newcommand{\maillink}[1]{\href{mailto:#1?subject=\mailsubject}
                               {#1}}

% Should use this command wherever the print date is mentioned.
\newcommand{\printdate}{\today}

\subject{451670 - Algorithmen, Wahrscheinlichkeit, Informationen}
\title{Series 5}

\author{Michael Senn \maillink{michael.senn@students.unibe.ch} - 16-126-880}

\date{\printdate}

% Needs to be the last command in the preamble, for one reason or
% another. 
\usepackage{hyperref}


\begin{document}
\maketitle


\setcounter{chapter}{4}
\chapter{Series 5}

\section{Fixed points of permutation}

Let $S = \{1, 2, \ldots, n\}$, let $\Pi$ be a random permutation of $S$.
Let:
\[
	I_{j} =
	\begin{cases}
		1 & \pi_j = j \\
		0 & \text{else}
	\end{cases}
\]

Then the number of fixed points in $\Pi$ is given as $X := \sum_{i=1}^n{I_i}$.

And:
\begin{align*}
	E[X] & = \sum_{i=1}^n{P(I_i = 1) * 1} = \sum_{i=1}^n{\frac{1}{n}} = 1 \\
	E[X^2] & = E\left[\left(\sum_{i=1}^n{P(I_i = i)}\right)^2\right] \\
	       & = E\left[\sum_{i=1}^n{\sum_{j=1}^n{P(I_i \cdot I_j = 1)}}\right] \\
\end{align*}

Note that:
\begin{align*}
	P(I_i \cdot I_j = 1) & = P(I_i \cdot I_j = 1 | i = j) + P(I_i \cdot I_j = 1 | i \neq j) \\
	\text{Where}: \\
	P(I_i \cdot I_j = 1 | i = j) & = \frac{1}{n} \\
	P(I_i \cdot I_j = 1 | i \neq j) & = P(I_i = 1) P(I_i \cdot I_j = 1 | I_i = 1) \\
					& = \frac{1}{n} \frac{1}{n-1}
\end{align*}

Hence:
\begin{align*}
	E[X^2] & = \sum_{i=1}^n{\frac{1}{n}} + \sum_{i=1}^n{\sum_{j=1}^{n-1}{\frac{1}{n} \frac{1}{n-1}}}
	& = 1 + 1 \\
	& = 2
\end{align*}

And finally:
\begin{align*}
	Var(X) = E[X^2] - E[X] = 2 - 1 = 1
\end{align*}

\section{Variance}

We aim to show that, for any random variable $X$ and constant $c > 0$
\[
	Var(cX) = c^2 Var(X)
\]
	
\begin{proof}

	\begin{align*}
		Var(cX) & = E[(cX)^2] - E[cX]^2 \\
			& = E[c^2 X^2] - E[cX]^2 \\
			& = c^2 E[X^2] - c^2 E[X]^2 \\
			& = c^2 (E[X^2] - E[X]^2) \\
			& = c^2 Var(X)
	\end{align*}
\end{proof}

\section{One-sided Chebyshev inequality}

We aim to show that, for any random variable $X \in \mathbb{N}$, with $Var(X) = \sigma^2$, and any $t > 0$:
\[
	P[X - E[X] \geq t \sigma] \leq \frac{1}{1 + t^2}
\]

\begin{proof}
	Let $Y := X - E[X]$. Then:
	\begin{align*}
		E[Y] & = E[X - E[X]] = E[X] - E[X] = 0 \\
		Var(Y) & = E[(Y - E[Y])^2] \\
		       & = E[(X - E[X] - E[X - E[X]])^2] \\
		       & = E[(X - E[X] - E[X] + E[X])^2] \\
		       & = E[(X - E[X])^2] \\
		       & = Var(X) = \sigma^2
	\end{align*}

	Then, for any $a > 0$, using Markov's inequality:
	\begin{align*}
		P(Y \geq t \sigma) & = P(Y + a \geq t \sigma + a) \\
				   & \leq \frac{E[Y + a]}{t \sigma + a} \\
		     & = \frac{a}{t \sigma + a}
	\end{align*}

	Adding $a$ is required as Markov's inequality requires a non-negative
	random variable. If $Y$ was non-negative, and $E[Y] = 0$, this would
	imply that $P(Y = 0) = 1$, and the inequality - while it would hold -
	would not add much value: $P(Y \geq t \sigma) \leq \frac{0}{t \sigma} = 0$.

	Specifically for $a := \frac{\sigma}{t}$:
	\begin{align*}
		P(Y + \frac{\sigma}{t} \geq t \sigma + \frac{\sigma}{t}) & \leq \frac{\frac{\sigma}{t}}{t \sigma + \frac{\sigma}{t}} \\
		& = \frac{\sigma}{t (t \sigma + \frac{\sigma}{t})} \\
		& = \frac{\sigma}{t^2 \sigma + \sigma} \\
		& = \frac{1}{t^2 + 1}
	\end{align*}

	Hence:
	\begin{align*}
		P(Y + \frac{\sigma}{t} \geq t \sigma + \frac{\sigma}{t}) & = P(Y \geq t \sigma) \\
		& = P(X - E[X] \geq t \sigma) \\
		& \leq \frac{1}{t^2 + 1}
	\end{align*}
\end{proof}

\section{Variance of collector's problem}

\subsection{Upper bound for variance}

As we have seen in the previous series, $X = \sum_{i=1}^n{X_i}$, with each
$X_i$ following a geometric distribution with $p = \frac{n-i+1}{n}$.

By the independence of the $X_i$ we get:
\begin{align*}
	Var(X) & = \sum_{i=1}^n{Var(X_i)} \\
	& = \sum_{i=1}^n{\frac{1 - \frac{n-i+1}{n}}{\left(\frac{n-i+1}{n}\right)^2}} \\
	& = \sum_{i=1}^n{\frac{n^2 - \frac{n^2 (n-i+1)}{n}}{n^2 \left(\frac{n-i+1}{n}\right)^2}} \\
	& = \sum_{i=1}^n{\frac{n^2 - n (n-i+1)}{(n-i+1)^2}} \\
	& \overset{0 < i \leq n}{<} \sum_{i=1}^n{\frac{n^2}{(n-i+1)^2}} \\
	& = n^2 \sum_{i=1}^n{\frac{1}{(n-i+1)^2}} \\
	& = n^2 \sum_{i=1}^n{\frac{1}{i^2}} \\
	& = \frac{n^2 \pi^2}{6}
\end{align*}

\subsection{Estimating probability of waiting four-times as long as normal}

Let $Var'(X)$ be our upper bound for the variance from the previous section.

\begin{align*}
	P[X \geq 4 E[X]] & = P(\abs{X - E[X]} \geq 3 E[X]) \\
	& \leq \frac{Var(X)}{(3 E[X])^2} \\
	& \leq \frac{Var'(X)}{(3 E[X])^2} \\
	& = \frac{\pi^2 n^2}{6 (3 n H(n))^2} \\
	& = \frac{\pi^2}{54 H(n)^2} \\
	& \leq \frac{\pi^2}{54 ln(n)^2}
\end{align*}


\end{document}

