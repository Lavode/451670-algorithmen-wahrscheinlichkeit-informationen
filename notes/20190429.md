# Information und Entropie

- Merke: Entropie == Unsicherheit
- Wie misst man Information?
- Wieviel Information steht in einer Nachricht?
=> Quantifizierung durch Informationstheorie (Shannon)

## Entropie

Die fundamentale Grösse ist Entropie einer ZV, ein Mass für die Unsicherheit,
oder Informationsgehalt, der ZV bezw spezifischer der Verteilungsfunktion der
Werte der ZV.

### Beispiel

- F ~ Un({0, 1}) (faire Münze, Bit, ...)
- W ~ Un({1, 2, 3, 4, 5, 6}) (fairer Würfel)
- Z in {0, a, b, ..., h}, wobei P(Z=0) = 1/2, P(Z=a) = ... = P(Z=h) = 1/16
- G ~ Ge(1/3) (Anzahl Experimente bis zum Erfolg), P(G=n) = (1-p)^(n-1) * p

### Informale Anforderungen an Informationsmass

- Zwei unabh Experimente, Information des kombinierten Experiments soll Summe
  der einzelnen Informationen sein
- Nichtnegativ
- Stetig (kleine Änderung der Verteilung soll kleine Verschiebung des
  Informationsmasses verursachen)
- Bezug auf oben: Z soll weniger Information haben als Z' mit P(Z'=0) = 3/4
- Gleichverteilung auf mehr Werte soll mehr Information enthalten als
  Gleichverteilung auf weniger Werte
- Gleichverteilung auf n Werte höchsten Informationsgehalt aller Verteilungen
  mit n Werten

### Versuch für Informationsmass

K:= log(|Supportmenge der ZV| = |{x in X: P(X=x) > 0}|)

- Log: Erlaubt Addition d Informationsmass unabh Experimente

Ab jetzt: Alle Log zu Basis 2 -> Messung der Informationen in Bits

- K(F) = 1
- K(W) = log(6) ~= 2.58
- K(Z) = log(9) ~= 3.12
- K(G) = log(infty) = infty

Ersichtlich: Schlechtes Informationsmass, P(G >= 100) ist sehr klein, also
nicht unendlich Information.

Idee: Infgehalt von X=x sei log(1/P(X=x)) (unwahrscheinliche Werte haben mehr Information)
-> Kommt auch nicht gut.

### Entropie (Shannon)

NB: Entropie != Information.

Def:
H(X) := Sum over x in X: P(X=x) * log(1/(P(X=x))) = -Sum over x in X: P(X=x) * log(P(X=x))

- Annahme: P(X=x) > 0 forall x in X
- Da Log zur Basis 2, misst Entropie in Bits

Alternative Def:
H(X) := E[-log P(X=x)]

### Entropie dr binären Verteilung

P[X=0] = p, P[X=1] = 1-p
h(p) = -p log(p) - (1-p) log(1-p)

Damit:
- Bei p=1/2 => h(p) = 1 bit
- Bei p=0.89 => h(p) = 0.5 bit

### Theorem

0 leq H(X) leq log(|X|), wobei:

a)
H(X) = 0 IFF P(X=x) = 1 für ein x

b)
H(X) = log(|X|) IFF P(X=x) = 1/|X| forall x (Uniformverteilung)

#### Bew

a)
Da P(X=x) > 0 forall x => 
  -P(X=x) log(P(X=x)) = 0 IFF P(X=x) = 0 für ein x
  -P(X=x) log(P(X=x)) > 0  IF 0 lt P(X=x) lt 1

b)
** PS: Jensen Ugl **
Jensen-Ugl: f(.) konkav => E[f(X)] leq f(E[X])

H(X) = E[-log(P(X=x))] = E[log(1/(P(X=x)))] leq log(E[1/P(X=x)]) = log(Sum over x in X: P(X=x) 1/P(X=x)) = log(|X|)

### Entropie der Verteilungen oben

- H(Z) = 2.5 
- H(W) = 2.58
- H(G) = h(p) / p  ~= 2.75
  - Übungsaufgabe

### Interpretationen von H(X)

- Anzahl nötiger Werte im Durchschnitt, um einen Wert von X zu codieren
- Anzahl binärer Fragen im Durchschnitt, um Resultat von X zu erfahren
- Anzahl perfekter Zufallsbits, die aus X im Durchschnitt extrahiert werden
  können (wenn X viele Male wiederholt, und Anzahl Bits je Wiederholung
  betrachtet)

### Bedingte Entropie

Def: Bedingte Entropie von X gegeben Ereignis Y = y:
  H(X|Y=y) = -Sum over x in X: P(X=X | Y=y) log(P(X=x | Y=y)) "Entropie von X | Y=y"

Def: Bedingte Entropie von X:
  H(X | Y) = Sum over y in Y: P(Y = y) * H(X | Y=y)

  Alterntive Definitionen:
    H(X | Y) = E[-log(P(X = x | Y))]
    H(X | Y)  = Sum over x in X: Sum over y in Y: P(X=x, Y=y) * log(P(X = x | Y=y))

### Bsp 1

X ~ Un({000, 001, ..., 111})
X = x1 x2 x3 (xi einzelne Bits)
Y = x1 XOR x2 XOR x3

H(X) = 3 (Gleichverteilt auf 2^3 Werte)
X(Y) = 1 (Gleichverteilt auf 2^1 Werte)
X(X | Y) = 2 (Intuitiv: 1 Bit bekannt durch Parity)

### Bsp 2

Barometer B in {Tief, Mittel, Hoch}
Wetter morgen W in {Sonnig, Wechselhaft, Regnerisch}

 W
B    s    w    r      P(B)
  t  0    1/12 2/12   1/4
  m  2/12 2/12 2/12   1/2
  h  2/12 0    1/12   1/4

P(W) 1/3  1/4  5/12

W'keiten "realistisch" gewählt.

Dann: H(B, W) ~= 1.675 bits
H(B) = H([1/4, 1/2, 1/4]) = 1.5
H(W) ~= 1.554
H(W | B=t) = h(1/3) ~= 0.918
H(W | B=m) = log(3) ~= 1.585 (Gleichverteilt)
  Bemerke: Unsicherheit grösser geworden (weil unwahrscheinliches Ereignis eingetreten)
  Konditionierung auf Ereignis kann Unsicherheit erhöhen.
H(W | B=h) = h(1/2) ~= 0.918
H(W | B) = 1/4 * H(W | B=t) + 1/2 * H(W = b=m) + 1/4 * H(W | B=h) ~= 1.252
  Unsicherheit über Wetter, gegeben dass wir den Barometer kennen, ist tiefer.
  Entspricht Fakt, dass die zwei ZV korreliert sind, und Wissen über die eine
  Informationen über die andere preisgibt

### Theorem

Für ZV X, Y gilt:

H(X | Y) leq H(X), mit Gleichheit gdw X, Y unabhängig

Interpretation: Zusatzinformationen verringern Unsicherheit

#### Bew

Später

### Theorem Kettenregel der Entropie

H(X, Y) = H(X) + H(Y | X) = H(Y) + H(X | Y)

### Wichtige Spezialfälle

1)
X unabhängig von Y
IFF H(X, Y) = H(X) + H(Y | X) = H(X) + H(Y)
IFF H(X | Y) = H(X)
IFF H(Y | X) = H(Y)

2)
Y = f(X) (deterministisch bestimmt)
IFF H(Y | X) = 0
IFF H(X, Y) = H(X)
