#### Bew (Shannon's Codierungs-Theorem)

Obere Schranke: Mit Shannon-Code:
```
w_i := ceil(-lb(P(S=s)))
E[W] = Sum for i = 1 to K: P(S=s_i) * w_i
     = Sum for i = 1 to K: P(S=s_i) * ceil(-lb(P(S=s_i)))
   leq Sum for i = 1 to K: P(S=s_i) * (-lb(P(S=s_i)) + 1)
     = -Sum for i = 1 to K: P(S=s_i) * lb(P(S=s_i)) + 1
     = H(S) + 1
```

Obere Schranke:
- Sei C optimaler binärer präfixfreier Code für S
- Erfüllt Kraft-Ungleichug mit Gleichheit, Sum for i = 1 to K: 2^(-w_i) = 1
- Sei U Quelle für welche P(U=s_i) = 2^(-w_i)
- Erwartete Codewortlänge:
```
E[W] = Sum for i = 1 to K: P(S=s_i) * w_i
     = Sum for i = 1 to K: P(S=s_i) * lb(2^w_i)
     = Sum for i = 1 to K: P(S=s_i) * (-lb(2^(-w_i)))
     = -Sum for i = 1 to K: P(S=s_i) * lb(P(U = s_i))
     // Gibbs Ugl
     \geq -Sum for i = 1 to K: P(S=s_i) * lb(P(S=s_i))
     = H(S)
```

### Anwendung

- Codierung kleine Datenmengen (zB E-Mail): Einmal durchgehen -> Alphabet und W'keiten -> Idealer Code
- Grosse / online Datenmengen: Kein "durchgehen" möglich -> zB LZ77 / LZ78, sliding window / dictionary

#### Bessere bounds

S^n codieren, dann 1/n * E[W] \leq H(S) + 1/n
da H(S^n) = n * H(S), falls einzelne S unabhängig

## Optimaler präfixfreie binäre Codes (Huffman-Codes)

- Quelle S in {s1, ..., sk}
- Binärer Code
- OBdA P(S=s_1) \geq ... \geq P(S=s_k)

Eigenschaften eines optimalen Code(-baumes):
- Hat keine unbenutzten Blätter
- Die zwei Symbole mit der kleinsten W'keit (s_k, s_(k-1)) unterscheiden sich nur im letzten Bit
  - Allgemein: Unwahrscheinliche Symbole unterscheiden sich in hinteren, wahrscheinliche in vorderen, Bits

- Seien s_i, s_j Symbole mit längstem Codewort
- Falls j := K tausche s_j und s_K
- Erhöht E[W] nicht, da P(S=s_j) \geq P(S=s_k), und w_j \geq w_K

- Falls s_i != s_(k-1), dann vertausche
- Erhöht E[W] nicht, weil P(S = s_(k-1)) \leq P(S = s_i), und w_(k-1) \leq w_i

### Huffman

- Bekannt aus DA.
- (s, p) Tupel, s Symbol d Alphabets, p W'keit
- Gruppiere zwei kleinste Knoten, Parent-Node = Summe der W'Keit
- Rinse and repeat

# Codierung mittels typischer Sequenzen

- Quelle, ZV S, Encoder, Code C
- Quelle besteht aus IID Wiederholungen einer ZV S.
- Code hat variable Länge

Hier:
- S als Bernoulli Quelle, P(S = 1) = p, P(S = 0) = 1 - p
- S^n unabh Wiederholungen von S
- Entropie H(S) = h(p) = -p * lb(p) - (1-p)*lb(1-p)
  - Notation: h(p) binäre Entropie
- H(S^n) = n * h(p)
- Code söll möglichst wenig, dh nahe an n*h(p), Bits brauchen (im Erwartungswert)

## Typische Sequenzen

Typsiche Sequenzen == Sequenzen mit `n * p` 1-en.
  Mit n -> infty schrumpt die W'keit dass eine andere Sequenz auftritt gegen 0

Total binom(k, p) Sequenzen mit k 1-einsen
Damit braucht der Code `ceil(lb(binom(n, np)))` bits

## Theorem

Sei S Bernoulli Quelle mit WSK p gt 1/2

Für delta gt 0, n genügend gross:
1) `E[W] \leq (1 + delta) * n * h(p)`
2) Für alle Codes: `E[W] \geq (1-delta) n * h(p)`

## Lemma

Sei q lt 1, sd n*q in N

```
2^(n * h(q)) / (n+1) \leq binom(n, n*q) \leq 2^(n*h(q))
```

### Bew

```
(q+r)^n = Sum for k = 0 to n: binom(n, k) q^k r^(n-k) // => r = 1-q
1 = (q + (1-q))^n = Sum for k = 0 to n: binom(n, k) q^k (1-q)^(n-k)
  \geq binom(n, nq) q^(nq) (1-q)^(n-nq)
=> q^(-nq) (1-q)^(-n(1-q))
  = 2^(log(q)^(-nq)) * (2^(log(1-q)))^(-n (1-q))
  = 2^(-nq * log(q)) * 2^(-n (1-q) log(1-q))
  = 2^(-n (q log(q) + (1-q) log(1-q)))
  = 2^(+n * h(q))

// Abschluss nächstes Mal; fehlendes `-` irgendwo

Ziel: Abschränkung gegen oben:

binom(n, nq) \leq 2^(n * h(q))


```

###
